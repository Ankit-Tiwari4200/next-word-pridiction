{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel aims to expand on what I've learned from the deeplearning.ai NLP Specialisation, one of the assignments for that course used **n-gram models** to build a auto-completion program. Most of this kernel is ported from that assignment. The dataset used for this kernel is very similar to the one used for the assignment.\n",
    "\n",
    "The aim is to convert this into a web application and deploy it to Heroku using streamlit. The Streamlit App can be found [here](https://autocomplete-ngram.herokuapp.com/) Takes a while to load 😅. Below is the screenshot of how the Application looks like. [Link to the Github repository](https://github.com/SauravMaheshkar/Auto-Completion-using-N-Gram-Models).\n",
    "\n",
    "![App Screenshot](https://raw.githubusercontent.com/SauravMaheshkar/Auto-Completion-using-N-Gram-Models/master/assets/app.png)\n",
    "\n",
    "####  If you liked this project and would like to read the code and see some of my other work, don't forget to ⭐ the [repository](https://github.com/SauravMaheshkar/Auto-Completion-using-N-Gram-Models) and follow [me](https://github.com/SauravMaheshkar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "\n",
    "* [📚 Theory](#section-one)\n",
    "    * [The Bigram Model ②](#bigram-model)\n",
    "    * [Estimation ⩰](#estimation)\n",
    "* [📂 Basic Setup](#basic-setup)\n",
    "* [🧽 Pre-Processing pipeline](#pre-process)\n",
    "* [✂️ Splitting into Train, Valid and Test](#split)\n",
    "* [🧹 Cleaning the Data](#clean)\n",
    "    * [📔 Creating a Frequency Dictionary](#frequency)\n",
    "    * [🔒 Creating a Closed Vocabulary](#closed) \n",
    "    * [🤷🏻 Adding UNK Tokens](#unk)\n",
    "    * [🧼 Final Cleaning Pipeline](#final)\n",
    "* [💪🏻 Building The \"Model\"](#build)\n",
    "* [💬 The Auto-Complete System](#auto-complete)\n",
    "* [😊 Inference](#inference)\n",
    "* [🧐 Miscellaneous](#misc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-one\"></a>\n",
    "# 📚 Theory\n",
    "\n",
    "Let's delve into the theory and try to gain a intuition about n-gram language models.\n",
    "\n",
    "N-Gram models are Statistical(Probabilistic) Language models that aim to assign probabilities to a given sequence of words. Any N-gram is just a sequence of \"n\" words. For example, \"Saurav\" is a unigram and \"Hi There\" is a bigram. \n",
    "\n",
    "\n",
    "The task is to find out if we can compute $P(w | h)$ given a word $w$ and some history $h$. One could say that we can compute the probability of a given next word, using all the previous words in the sentence. For example using the last sentence, we could calculate: \n",
    "\n",
    "$$\\large\n",
    "P ( \\, word \\, | \\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\,)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "One such approach could be to use **relative frequency counts** to compute this probability, i.e. ,**Out of the times we saw the history $h$, how many times was it followed by the word $w$**\n",
    "\n",
    "Or \n",
    "\n",
    "$$\n",
    "P ( \\, word \\, | \\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\,) = \\frac{C(\\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\, word)}{C(\\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\,)}\n",
    "$$\n",
    "---\n",
    "\n",
    "Intuitively it seems infeasible to perform this over an entire corpus; especially it is of a significant a size. This is the motivation behind the N-gram model, instead of using the entire corpus, we approximate this probability using just `n` previous words.\n",
    "\n",
    "For instance if $w_{1:n}$ represents the sequence of words $w_1w_2...w_n$, then using the chain rule of probability we can write,  \n",
    "\n",
    "\n",
    "$$\\large\n",
    "P(w_{1:n}) = P(w_1)P(w_2 | w_1)P(w_3 | w_{1:2})...P(w_n|w_{1:n-1})\n",
    "$$\n",
    "\n",
    "\n",
    "$$\\large\n",
    "P(w_{1:n}) = \\prod_{k=1}^{n}P(w_k | w_{1:k-1})\n",
    "$$\n",
    "\n",
    "<a id=\"bigram-model\"></a>\n",
    "## The Bigram Model ②\n",
    "\n",
    "A Bigram Model corresponds to a model which approximates the probability of a word given all the previous words $P(w_n|w_{1:n−1})$ by using only the conditional probability of the preceding word $P(w_n|w_{n−1})$. Thus we assume that $P(w_n|w_{1:n−1}) ≈ P(w_n|w_{n−1})$. This approximation is known as the **Markov** approximation. Thus, for the Bigram model, the probability for an entire sequence can be approximated as:\n",
    "\n",
    "$$\\large\n",
    "P(w_{1:n}) ≈ \\prod_{k=1}^{n}P(w_{k}|w_{k−1}) \n",
    "$$\n",
    "\n",
    "<a id=\"estimation\"></a>\n",
    "## Estimation ⩰\n",
    "\n",
    "To estimate such probabilities we use the **Maximum Likelihood Estimation (MLE)**. An MLE estimate for the parameters of an n-gram model can be obtained by getting counts from a corpus, and normalizing the counts so that they lie between 0 and 1.\n",
    "\n",
    "For a Bigram model, the MLE Estimation can be given by:\n",
    "\n",
    "$$\\large\n",
    "P(w_n | w_{n-1}) \\frac{C(w_{n-1}w_n)}{\\sum_{w} C(w_{n-1}w)}\n",
    "$$\n",
    "\n",
    "---\n",
    "For the general case of MLE n-gram parameter estimation:\n",
    "\n",
    "$$\\large\n",
    "P(w_n|w_{n−N+1:n−1}) = \\frac{C(w_{n−N+1:n−1}w_n)}{C(w_{n−N+1:n−1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = \"../input/figlang-train\"\n",
    "file_name =  \"/train.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(input_data) -> 'list':\n",
    "\n",
    "    sentences = input_data.split('\\n') # --> pisah berdasarkan '\\n'\n",
    "    \n",
    "    sentences = [s.strip() for s in sentences] # --> hapus spasi \n",
    "    \n",
    "    sentences = [s for s in sentences if len(s) > 0] # --> buang kalimat yang gada isinya\n",
    "    \n",
    "    tokenized = [] # --> variabel untuk menampung kalimat yang sudah diproses menjadi token token nantinya \n",
    "    \n",
    "    for sentence in sentences: # --> looping per kalimat\n",
    "        \n",
    "        sentence = sentence.lower() # --> seluruh kalimat dibuat menjadi lowercase \n",
    "        \n",
    "        token = nltk.word_tokenize(sentence) # kalimat dijadikan sebagai token yang nantinya akan distore pada array yang sudah dideclare\n",
    "        \n",
    "        tokenized.append(token) # masukkan ke dalam array \n",
    "        \n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenized_sentences = preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(tokenized_sentences, test_size=0.2, random_state=42) # -> pemisahan dataset untuk training set dan testing set \n",
    "\n",
    "train_set, val_set = train_test_split(train_set, test_size=0.25, random_state=42) # -> pemisahan dataset untuk training set dan validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(sentences) -> 'dict':\n",
    "    \n",
    "    count = {} # dictionary untuk store frekuensi kemunculan sebuah kata \n",
    "    for sentence in sentences: # looping untuk seluruh kalimat \n",
    "    \n",
    "        for token in sentence: # looping setiap token\n",
    "    \n",
    "            if token not in count.keys(): # jika tidak ada di dictionarynya maka dianggap sebagai kata baru\n",
    "                count[token] = 1\n",
    "        \n",
    "            else: # jika sudah ada, increment saja \n",
    "                count[token] += 1\n",
    "        \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_vocab(tokenized_sentences, count_threshold) -> 'list':\n",
    "    \n",
    "    closed_vocabulary = [] # list untuk kosakata tertutup\n",
    "    temp = word_count(tokenized_sentences) # hitung frekuensi dari sentence yang diinput \n",
    "    \n",
    "    for word, count in temp.items(): # looping untuk setiap kata dan count yang sudah di assign \n",
    "        if count >= count_threshold : # kalau melebihi threshold / batas, masukan ke list \n",
    "                closed_vocabulary.append(word)\n",
    "\n",
    "    return closed_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown(tokenized_sentences, vocabulary, unknown_token = \"<unknown>\") -> 'list':\n",
    "\n",
    "  # Ubah menjadi set \n",
    "    vocabulary = set(vocabulary)\n",
    "\n",
    "  # Buat list baru yang akan digunakan untuk store vocab baru \n",
    "    new_tokenized_sentences = []\n",
    "  \n",
    "  # Looping untuk setiap kalimat \n",
    "    for sentence in tokenized_sentences:\n",
    "\n",
    "    # Looping setiap kalimat dan diberi label <unknown> untuk unknown \n",
    "    # Apabila di \"library\" kita tidak tersedia kata tersebut\n",
    "        new_sentence = []\n",
    "        for token in sentence:\n",
    "            if token in vocabulary:\n",
    "                new_sentence.append(token)\n",
    "            else:\n",
    "                new_sentence.append(unknown_token)\n",
    "    \n",
    "    # Tambahkan kalimat atau kata ke dalam list \n",
    "        new_tokenized_sentences.append(new_sentence)\n",
    "    return new_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(train_data, test_data, count_threshold):\n",
    "    \n",
    "  # ambil kosakata tertutup\n",
    "    vocabulary = closed_vocab(train_data, count_threshold)\n",
    "    \n",
    "  # update training set\n",
    "    new_train_data = unknown(train_data, vocabulary)\n",
    "    \n",
    "  # update testing set\n",
    "    new_test_data = unknown(test_data, vocabulary)\n",
    "\n",
    "    return new_train_data, new_test_data, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 6\n",
    "final_train, final_test, vocabulary = cleansing(train_set, test_set, min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token = \"<s>\", end_token = \"<e>\") -> 'dict':\n",
    "\n",
    "  # Empty dict for n-grams\n",
    "    n_grams = {}\n",
    " \n",
    "  # Iterate over all sentences in the dataset\n",
    "    for sentence in data:\n",
    "        \n",
    "    # Append n start tokens and a single end token to the sentence\n",
    "        sentence = [start_token]*n + sentence + [end_token]\n",
    "    \n",
    "    # Convert the sentence into a tuple\n",
    "        sentence = tuple(sentence)\n",
    "\n",
    "    # Temp var to store length from start of n-gram to end\n",
    "        m = len(sentence) if n==1 else len(sentence)-1\n",
    "    \n",
    "    # Iterate over this length\n",
    "        for i in range(m):\n",
    "        \n",
    "      # Get the n-gram\n",
    "            n_gram = sentence[i:i+n]\n",
    "    \n",
    "      # Add the count of n-gram as value to our dictionary\n",
    "      # IF n-gram is already present\n",
    "            if n_gram in n_grams.keys():\n",
    "                n_grams[n_gram] += 1\n",
    "      # Add n-gram count\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "        \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the priority for the next word given the prior n-gram. This function also implements k-smoothing which helps account for unseen n-grams. Using the previously defined formula:\n",
    "\n",
    "\n",
    "$$\\large\n",
    "P(w_n|w_{n−N+1:n−1}) = \\frac{C(w_{n−N+1:n−1}w_n)}{C(w_{n−N+1:n−1})}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### K-smoothing\n",
    "\n",
    "But what if we come across a n-gram that wasn't in the training set. Then our denominator would would become zero and our definition of probability will become invalid. Thus, we use k-smoothing, which adds a positive constant $k$ to each numerator and $k \\times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary. This ensures any n-gram with zero count has the same probability of $\\frac{1}{|V|}$. Thus, our original estimation get's modified to:\n",
    "\n",
    "$$\\large\n",
    "P(w_n|w_{n−N+1:n−1}) = \\frac{C(w_{n−N+1:n−1}w_n) + k}{C(w_{n−N+1:n−1} + k |V|)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_for_single_word(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size, k = 1.0) -> 'float':\n",
    "\n",
    "  # Convert the previous_n_gram into a tuple \n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "  # Calculating the count, if exists from our freq dictionary otherwise zero\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "  \n",
    "  # The Denominator\n",
    "    denom = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "  # previous n-gram plus the current word as a tuple\n",
    "    nplus1_gram = previous_n_gram + (word,)\n",
    "\n",
    "  # Calculating the nplus1 count, if exists from our freq dictionary otherwise zero \n",
    "    nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n",
    "\n",
    "  # Numerator\n",
    "    num = nplus1_gram_count + k\n",
    "\n",
    "  # Final Fraction\n",
    "    prob = num / denom\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we loop over all the words in the vocabulary and then compute their probabilites using our `prob_for_single_word()` fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0) -> 'dict':\n",
    "\n",
    "  # Convert to Tuple\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "  # Add end and unknown tokens to the vocabulary\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "\n",
    "  # Calculate the size of the vocabulary\n",
    "    vocabulary_size = len(vocabulary)\n",
    "\n",
    "  # Empty dict for probabilites\n",
    "    probabilities = {}\n",
    "\n",
    "  # Iterate over words \n",
    "    for word in vocabulary:\n",
    "    \n",
    "    # Calculate probability\n",
    "        probability = prob_for_single_word(word, previous_n_gram, \n",
    "                                           n_gram_counts, nplus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "    # Create mapping: word -> probability\n",
    "        probabilities[word] = probability\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"auto-complete\"></a>\n",
    "# 💬 The Auto-Complete System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we build our `auto_complete` fn. We simply loop over all the words in the vocabulary assuming that they can be the next word and then return the word with it's probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_complete(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "\n",
    "    \n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    # most recent 'n' words\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    # Calculate probabilty for all words\n",
    "    probabilities = probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary, k=k)\n",
    "\n",
    "    # Intialize the suggestion and max probability\n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "\n",
    "    # Iterate over all words and probabilites, returning the max.\n",
    "    # We also add a check if the start_with parameter is provided\n",
    "    for word, prob in probabilities.items():\n",
    "        \n",
    "        if start_with != None: \n",
    "            \n",
    "            if not word.startswith(start_with):\n",
    "                continue \n",
    "\n",
    "        if prob > max_prob: \n",
    "\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "\n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also loop over all the various n-gram models to get multiple suggestions. This function just extends from the previously defined function by **taking multiple n-gram counts** instead of one. This allows us to take unigram, bigram, .. counts into account as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "\n",
    "    # See how many models we have\n",
    "    count = len(n_gram_counts_list)\n",
    "    \n",
    "    # Empty list for suggestions\n",
    "    suggestions = []\n",
    "    \n",
    "    # IMP: Earlier \"-1\"\n",
    "    \n",
    "    # Loop over counts\n",
    "    for i in range(count-1):\n",
    "        \n",
    "        # get n and nplus1 counts\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        nplus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        # get suggestions \n",
    "        suggestion = auto_complete(previous_tokens, n_gram_counts,\n",
    "                                    nplus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        # Append to list\n",
    "        suggestions.append(suggestion)\n",
    "        \n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"inference\"></a>\n",
    "# 😊 Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a list of n-gram counts for a arbitrary range `(1,6)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    n_model_counts = count_n_grams(final_train, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a sample input of \"i was about\" in a tokenized manner and get multiple suggestions using the above calculated n-gram counts with smoothing-factor, `k` = 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"this\", \"is\", \"my\"]\n",
    "suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "display(suggestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"misc\"></a>\n",
    "# 🧐 Miscellaneous "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many n-grams we have in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"unigram count:\" , len(n_gram_counts_list[0]))\n",
    "print(\"bigram count:\", len(n_gram_counts_list[1]))\n",
    "print(\"trigram count:\", len(n_gram_counts_list[2]))\n",
    "print(\"quadgram count:\", len(n_gram_counts_list[3]))\n",
    "print(\"quintgram count:\", len(n_gram_counts_list[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we just export this list to a `.txt` file so that we can use this for inference rather than \"training\" each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing to file\n",
    "with open(\"en_counts.txt\", 'wb') as f:\n",
    "    pickle.dump(n_gram_counts_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing to file\n",
    "with open(\"vocab.txt\", 'wb') as f:\n",
    "    pickle.dump(vocabulary, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
